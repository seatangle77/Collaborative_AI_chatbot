import os
import json
from openai import OpenAI  
from dotenv import load_dotenv

# âœ… åŠ è½½ .env é…ç½®
load_dotenv()

# âœ… è¯»å– API ç›¸å…³é…ç½®
XAI_API_KEY = os.getenv("XAI_API_KEY")
XAI_API_BASE = os.getenv("XAI_API_BASE", "https://api.x.ai/v1")

# âœ… åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI(
    api_key=XAI_API_KEY,
    base_url=XAI_API_BASE,
)

# âœ… è·å– `prompt.json` çš„æ­£ç¡®è·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # è·å–å½“å‰ `xai_api.py` æ‰€åœ¨çš„ç›®å½•
PROMPT_PATH = os.path.join(BASE_DIR, "prompt.json")  # ç”Ÿæˆ `prompt.json` çš„å®Œæ•´è·¯å¾„

# âœ… è¯»å– JSON é…ç½®ï¼Œç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®
try:
    with open(PROMPT_PATH, "r", encoding="utf-8") as f:
        prompt_config = json.load(f)
except FileNotFoundError:
    print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° `prompt.json` æ–‡ä»¶ï¼Œæ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼š{PROMPT_PATH}")
    prompt_config = {}  # é¿å…åç»­ `KeyError`ï¼Œä½†ä¼šå¯¼è‡´ API æ— æ³•æ­£ç¡®è¿è¡Œ

def generate_ai_response(main_prompt: str, history_prompt: str = None, prompt_type: str = "real_time_summary", model: str = "grok-2-latest"):
    """
    å‘é€è¯·æ±‚åˆ° xAI APIï¼ŒåŸºäº prompt_type é€‰æ‹©ä¸åŒçš„æç¤ºè¯ (prompt)
    """
    if prompt_type not in prompt_config:
        return f"âŒ æœªæ‰¾åˆ° '{prompt_type}' å¯¹åº”çš„ promptï¼Œè¯·æ£€æŸ¥ `prompt.json`ã€‚"

    try:
        # âœ… è¯»å–å¯¹åº”ç±»å‹çš„ prompt
        prompt_data = prompt_config[prompt_type]
        max_words = prompt_data["max_words"]
        system_prompt = prompt_data["system_prompt"].replace("{max_words}", str(max_words))

        # âœ… å¤„ç†ä¸åŒçš„ `prompt_type`
        if prompt_type == "real_time_summary":
            user_prompt = f"è¯·åœ¨ {max_words} è¯ä»¥å†…æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼š\n\n{main_prompt}"
        elif prompt_type == "cognitive_guidance":
            user_prompt = f"è¯·æ ¹æ®ä»¥ä¸‹è®¨è®ºå†…å®¹ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦å¼•å¯¼å›¢é˜Ÿè¿›ä¸€æ­¥è®¨è®ºï¼Œå¹¶æä¾›çŸ¥è¯†æ”¯æŒï¼š\n\n{main_prompt}"
        elif prompt_type == "term_explanation":
            user_prompt = f"è¯·åœ¨ {max_words} è¯ä»¥å†…è§£é‡Šè¿™ä¸ªæœ¯è¯­ï¼š\n\n{main_prompt}"
        else:
            return "âŒ ä¸æ”¯æŒçš„ `prompt_type`"

        messages = [
            {"role": "system", "content": system_prompt},  
            {"role": "user", "content": user_prompt},  
        ]

        # âœ… å¦‚æœæœ‰å†å²æ¶ˆæ¯ï¼Œåˆ™æ’å…¥ `assistant` è§’è‰²ï¼ˆä½†æƒé‡è¾ƒä½ï¼‰
        if history_prompt:
            messages.append({"role": "assistant", "content": history_prompt})

        # âœ… æ„é€  API è¯·æ±‚æ•°æ®
        api_payload = {
            "model": "grok-2-latest",
             "temperature": 1,
            "messages": messages,
            "max_tokens": 280,

        }

        # âœ… **æ‰“å°å³å°†å‘é€çš„ API è¯·æ±‚å‚æ•°**
        print("ğŸ“¤ model:")
        print("ğŸ“¤ å‘é€è¯·æ±‚åˆ° xAI API:")
        print(f"ğŸ”— API ç½‘å€: {XAI_API_BASE}")
        print(f"ğŸ”‘ API Key: {'âœ… å·²è®¾ç½®' if XAI_API_KEY else 'âŒ æœªè®¾ç½®'}")
        print(f"ğŸ“¦ è¯·æ±‚ Payload:\n{json.dumps(api_payload, indent=2, ensure_ascii=False)}")

        # âœ… å‘é€ API è¯·æ±‚
        response = client.chat.completions.create(**api_payload)

        # âœ… **æ‰“å° API è¿”å›ç»“æœ**
        print(f"ğŸ“¥ API å“åº”: {response}")

        ai_text = response.choices[0].message.content.strip()
        print(f"âœ… xAI API å“åº”:\n{ai_text}")  # âœ… æ‰“å° AI ç”Ÿæˆçš„å†…å®¹
        return ai_text
    except Exception as e:
        print(f"âŒ xAI API è¯·æ±‚å¤±è´¥: {e}")
        return "AI ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚"